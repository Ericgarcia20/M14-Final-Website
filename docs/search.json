[
  {
    "objectID": "strategic-marketing.html",
    "href": "strategic-marketing.html",
    "title": "Strategic Marketing Projects",
    "section": "",
    "text": "Below are selected projects that showcase my work in strategic marketing, campaign development, and brand management. These examples highlight how I apply leadership, data-informed decision making, and creative direction to drive marketing success."
  },
  {
    "objectID": "strategic-marketing.html#leveraging-artificial-intelligence-ai-through-active-listening-training",
    "href": "strategic-marketing.html#leveraging-artificial-intelligence-ai-through-active-listening-training",
    "title": "Strategic Marketing Projects",
    "section": "Leveraging Artificial Intelligence (AI) Through Active Listening Training",
    "text": "Leveraging Artificial Intelligence (AI) Through Active Listening Training\nIn this project, I led the development of a comprehensive AI-driven training tool to improve salesperson listening skills using GPT-based chatbots. My role centered around designing and deploying two distinct GPT chatbot models (one that accepts free-text responses and another that uses multiple-choice interactions) to simulate real-life B2B sales scenarios and evaluate salesperson behavior. I integrated prompt engineering techniques to measure key communication tactics and constructed over 1,000 survey-style prompts to gather measurable insights on listener effectiveness.\nAs part of the analytical objective, I applied a custom GPT formula across six targeted questions, 50 buyer personas, and four experimental conditions, resulting in 200 unique persona-condition combinations and generating a dataset of over 1,000 structured response points. These simulated conversations were then analyzed to identify which listening tactics (Delay = timing of response) and (Value Affirmation = emphasizing relevance of features) influenced the Purchase Decision (dependent variable).\nThe six questions, structured as Likert-scale and binary prompts (e.g., “Did the salesperson have a delay in their interaction?”), helped evaluate behaviors like timing, affirmation, and persistence during negotiations. Survey ratings were derived strictly using the customer’s persona, final response, and rationale, providing standardized data for comparison across conditions.\nThis work is part of a broader research initiative aimed at bridging the gap between subjective listening skills and objective performance in sales calls. Through careful documentation, iterative prompt refinement, and collaboration with a partner, we designed a scalable, cost-effective solution to train and evaluate salespeople using generative AI tools.\nSkills Demonstrated:\n\nAI Integration & Prompt Engineering: Designed structured and variable prompts across complex conversational conditions to model sales interactions and behaviors.\nData Analysis & Experimental Design: Implemented controlled variables (Delay, Value Affirmation) and collected over 1,000 data points across 200 chatbot simulations.\nMarketing & Training Strategy: Created a training tool rooted in real-world sales psychology to improve sales performance using LLM-based feedback.\nCollaborative Development: Worked closely with a teammate to co-develop chatbot logic, refine output, and maintain detailed project documentation.\nUX & Conversational Design: Crafted free-form and multiple-choice interfaces to simulate sales conversations and deliver user-friendly training flows."
  },
  {
    "objectID": "strategic-marketing.html#protein-popcorn-chatbot---group-client-research",
    "href": "strategic-marketing.html#protein-popcorn-chatbot---group-client-research",
    "title": "Strategic Marketing Projects",
    "section": "Protein Popcorn Chatbot - Group Client Research",
    "text": "Protein Popcorn Chatbot - Group Client Research\nIn this project, I led the development of a comprehensive marketing research and strategy initiative for Up Energy’s Protein Popcorn chatbot recommendation tool. Focused on improving customer acquisition and product awareness, I spearheaded a cross-functional campaign that involved in-depth consumer research, segmentation analysis, and strategic recommendations. From identifying decision-making challenges to exploring digital marketing avenues, my team and I managed the entire lifecycle of the project, aligning our strategy with client business goals and KPIs related to consumer engagement, conversion rates, and brand positioning.\nOur work included assessing consumer familiarity with protein popcorn, evaluating perceptions around its health benefits, and understanding how messaging, flavors, packaging, and pricing influenced buyer behavior. We also studied how demographic factors and fitness levels shaped preferences, and made targeted recommendations to help the brand reach beyond typical gym-goers to appeal to broader audiences like parents, busy professionals, and individuals with dietary restrictions.\nUltimately, the chatbot was envisioned as an intelligent, data-driven tool to support personalized product recommendations based on consumer profiles, thereby improving the user experience and helping the brand stand out in a competitive healthy snack market.\nSkills Demonstrated:\n\nMarketing Strategy: Defined business objectives around consumer engagement and product adoption. Created data-driven strategies to meet those goals, including targeted outreach and segmentation.\nMarket Research: Conducted qualitative and quantitative research, including consumer interviews and sentiment analysis, to evaluate brand awareness, packaging preferences, pricing sensitivity, and dietary needs.\nConsumer Insights: Identified behavioral patterns and purchase drivers across demographics, fitness levels, and income groups to craft more inclusive marketing strategies.\nMessaging and Positioning: Developed storytelling frameworks emphasizing transparency, science-backed health claims, and real-world usability to enhance brand trust and relatability.\nDigital Marketing Tactics: Recommended high-impact digital channels such as Instagram, influencer partnerships, and sampling events to boost brand visibility and product trials.\nCross-functional Collaboration: Worked with a diverse team to synthesize insights, brainstorm innovative solutions, and deliver a cohesive, actionable strategy to the client."
  },
  {
    "objectID": "strategic-marketing.html#farm-store-retail-strategy",
    "href": "strategic-marketing.html#farm-store-retail-strategy",
    "title": "Strategic Marketing Projects",
    "section": "Farm Store Retail Strategy",
    "text": "Farm Store Retail Strategy\nIn this project, I led the development of retail experience enhancements for the Cal Poly Pomona Farm Store, with a focus on HR optimization, customer service design, and store presentation. I contributed to the merchandising strategy through the creation of a themed gift box concept tailored to the store’s seasonal retail events, emphasizing local branding and customer engagement. My responsibilities included redesigning the store’s visual merchandising approach to address layout constraints and improve traffic flow, as well as introducing service strategies to better align with customer expectations and staffing limitations.\nAdditionally, I developed practical solutions to enhance store management, incorporating workforce planning that considers student staffing cycles and seasonal demand. By aligning store presentation with the Farm Store’s value proposition (offering fresh, locally grown products in a student-driven retail environment) I supported both the educational mission and commercial success of the store.\nSkills Demonstrated:\n\nRetail Strategy & Merchandising: Created a thematic gift box concept aligned with seasonal promotions, improving impulse purchases and gift revenue streams.\nVisual Merchandising: Designed store layout and presentation strategies to optimize limited physical space and improve the in-store customer journey.\nHuman Resource Management: Proposed staffing solutions suited for a student-operated store, addressing turnover, seasonality, and scheduling challenges.\nCustomer Experience Design: Enhanced service models to reduce crowding frustrations, incorporating clearer signage, better queue management, and convenience-focused adjustments.\nBrand Development: Integrated farm-to-table identity and Cal Poly branding into packaging and presentation to differentiate the Farm Store from commercial competitors."
  },
  {
    "objectID": "strategic-marketing.html#online-coffee-retailer-on-shopify",
    "href": "strategic-marketing.html#online-coffee-retailer-on-shopify",
    "title": "Strategic Marketing Projects",
    "section": "Online Coffee Retailer on Shopify",
    "text": "Online Coffee Retailer on Shopify\nIn this project, I led the development of a complete business plan for iluvcafecito, an e-commerce coffee brand under TJE Holdings, LLC. I was responsible for shaping the concept, building the brand identity, and creating a comprehensive strategy covering operations, marketing, product offerings, and financial forecasting. From identifying the target market to crafting the brand’s mission and visual style, I directed all aspects of the business launch plan with an emphasis on sustainability, ethical sourcing, and customer-centric innovation.\nThe plan included a multi-phase rollout starting with online sales of premium coffee products (beans, grounds, accessories, and subscriptions) followed by pop-up presence at local farmers markets, and ultimately the establishment of a physical retail location. I designed the customer experience strategy around convenience and customization through tailored subscription models, while building in retention tactics like loyalty programs and seasonal product drops.\nSkills Demonstrated:\n\nBusiness Strategy & Planning: Developed a scalable business model with phased growth from e-commerce to brick-and-mortar, with defined milestones and financial projections.\nBrand Development: Created a unique brand identity that emphasizes ethical sourcing, sustainability, and premium quality, with cohesive branding across packaging, digital presence, and marketing.\nMarketing Strategy: Outlined a full-funnel marketing approach including SEO, content marketing, social media campaigns, influencer partnerships, and paid advertising to drive traffic and conversion.\nE-Commerce Development: Designed the e-commerce platform architecture (Shopify), including payment integration, product organization, and subscription customization.\nOperations Management: Established supplier relationships (e.g., Temecula Coffee Roasters), fulfillment plans, customer service protocols, and technology stack including CRM and email automation tools.\nFinancial Forecasting: Built realistic financial models projecting revenue growth, break-even timelines, and funding requirements, with a focus on recurring subscription income and high-margin upsells."
  },
  {
    "objectID": "resume/Resume.html",
    "href": "resume/Resume.html",
    "title": "Professional Summary",
    "section": "",
    "text": "Professional Summary\nMotivated and skilled digital marketer and administrator with nearly 2 years of digital marketing experience and over 6 years of professional expertise across small businesses and corporate environments. Passionate about leveraging leadership to drive success in SEO, SEM, and content management while contributing to organizational growth and innovation.\n\n\nTechnical Skills\n\nProgramming & Tools: R, SQL, RStudio, MS Word/Excel/PowerPoint, Canva, Adobe Photoshop, Acrobat\nMarketing & Analytics: Google Analytics (Certified), HubSpot (Certified), SEMrush, Qualtrics, Social Media\nE-commerce Platforms: Amazon Seller Central, Walmart Seller Center, eBay Seller Center\nOther Software: ADP, Opera Cloud, SAGE\n\n\n\nEducation\n\n\n# A tibble: 2 × 5\n  what                                 when          with            where why  \n  &lt;chr&gt;                                &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt; &lt;lis&gt;\n1 MS in Digital Marketing              Expected 2025 College of Bus… Cali… &lt;chr&gt;\n2 BA in Sociology (Minor: Criminology) 2022          College of Let… Cali… &lt;chr&gt;\n\n\n\n\nWork Experience\n\n\n# A tibble: 6 × 5\n  what                               when                with       where why   \n  &lt;chr&gt;                              &lt;chr&gt;               &lt;chr&gt;      &lt;chr&gt; &lt;list&gt;\n1 Digital Marketing Specialist       Aug 2024 – Present  Durimex I… &lt;NA&gt;  &lt;list&gt;\n2 Independent Consultant, Internship May 2024 – Jul 2024 Parker De… &lt;NA&gt;  &lt;list&gt;\n3 Marketing Support / Administration Feb 2023 – Sep 2024 LA MERE, … &lt;NA&gt;  &lt;list&gt;\n4 Account Training Manager           Aug 2022 – Dec 2022 Strategic… &lt;NA&gt;  &lt;list&gt;\n5 Level 1 Associate                  Aug 2020 – Aug 2022 Amazon So… &lt;NA&gt;  &lt;list&gt;\n6 Employee / Delivery Driver         Jun 2018 – May 2020 New York … &lt;NA&gt;  &lt;list&gt;\n\n\n\n\nProjects\n\n\n# A tibble: 5 × 3\n  what                                                            when  with \n  &lt;chr&gt;                                                           &lt;chr&gt; &lt;chr&gt;\n1 Recovered $60,000+ for La Mere, LLC from Riverside County DPSS. &lt;NA&gt;  &lt;NA&gt; \n2 Recovered $30,000+ owed from hotel sale to Ayaan Pithiya, LLC.  &lt;NA&gt;  &lt;NA&gt; \n3 Collected $20,000+ in damages from CDPH for La Mere, LLC.       &lt;NA&gt;  &lt;NA&gt; \n4 Located $40,000 in escrow deposits during hotel sale.           &lt;NA&gt;  &lt;NA&gt; \n5 Resolved $6,000 in invoice discrepancies for La Mere, LLC.      &lt;NA&gt;  &lt;NA&gt; \n\n\n\n\nCertifications & Training\n\nStukent – Digital Marketing Simternship (Completed)\nHubSpot – Inbound Marketing & Marketing Software Certifications\nSEMrush – SEO Principles Certification\nGoogle Analytics – Certified"
  },
  {
    "objectID": "listings/05-advertising-channel-effectiveness.html",
    "href": "listings/05-advertising-channel-effectiveness.html",
    "title": "Advertising Channel Effectiveness Analysis",
    "section": "",
    "text": "Overview\nThis analysis explores how advertising investments across various channels (such as TV, Google Ads, Social Media, and more) impact product sales. By visualizing spending patterns and running a predictive model, we identify the most influential marketing strategies driving performance.\n\n\n\n\nLearning Outcomes\n\nImport, clean, and transform advertising datasets.\n\nExplore channel-specific sales impact using correlation and regression.\n\nVisualize multi-channel advertising effectiveness.\n\nGenerate actionable insights for campaign optimization.\n\nPresent findings in a clear, data-driven summary.\n\n\n\n\n\nKey Skills Gained\n\nData wrangling with tidyverse\n\nCorrelation and regression analysis\n\nData visualization with ggplot2\n\nMarketing performance modeling\n\nStrategic advertising recommendations\n\n\n\n\n\n\nData-set Used\nClick Here for Kaggle Data-set\n\n# Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\n\n\n# Data\ndata &lt;- read.csv(\"Advertising_Data.csv\")\n\n\n# Correlation Heatmap\ncorr_matrix &lt;- cor(data)\ncorrplot(corr_matrix, method = \"color\", type = \"upper\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nDescription: This heatmap visualizes the pairwise correlation between all advertising channels and product sales. A value close to 1 indicates a strong positive correlation, while values close to -1 suggest a strong negative relationship. This plot helps identify which advertising investments are most closely related to higher product sales.\nUse Case: Quickly assess which channels might be more effective in influencing product performance.\n\n# Scatter Plots\ndata_long &lt;- data %&gt;%\n  pivot_longer(cols = -Product_Sold, names_to = \"Channel\", values_to = \"Spend\")\n\nggplot(data_long, aes(x = Spend, y = Product_Sold)) +\n  geom_point(alpha = 0.6, color = \"#0073C2FF\") +\n  facet_wrap(~ Channel, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Advertising Spend vs. Product Sold\", x = \"Advertising Spend\", y = \"Product Sold\")\n\n\n\n\n\n\n\n\nDescription: This set of scatter plots displays the relationship between spending in each advertising channel and the number of products sold. Each plot shows how variations in spending may correspond to sales volume for that specific channel.\nUse Case: Visually inspect trends, patterns, or outliers for each advertising medium and how it might drive sales.\n\n# Multiple Linear Regression\nmodel &lt;- lm(Product_Sold ~ ., data = data)\ndata$Predicted_Sales &lt;- predict(model, data)\n\nggplot(data, aes(x = Product_Sold, y = Predicted_Sales)) +\n  geom_point(color = \"#EFC000FF\", alpha = 0.7) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Actual vs. Predicted Product Sales\", x = \"Actual Sales\", y = \"Predicted Sales\")\n\n\n\n\n\n\n\n\nDescription: This plot compares the actual number of products sold to the values predicted by a linear regression model using all advertising channels as predictors. The dashed red line represents a perfect match between predicted and actual sales.\nUse Case: Evaluate the overall predictive accuracy of your regression model and assess how well advertising spend explains sales performance."
  },
  {
    "objectID": "listings/03-logistic-regression-pred.html",
    "href": "listings/03-logistic-regression-pred.html",
    "title": "Titanic Survival Prediction with Logistic Regression",
    "section": "",
    "text": "Overview\nThis project developed a logistic regression model to predict Titanic passenger survival based on demographics and travel class. The analysis included preprocessing, handling class imbalance with SMOTE, and evaluating model performance using accuracy, sensitivity, and specificity metrics.\n\n\n\n\nLearning Outcomes\n\nImport and clean real-world survival data.\n\nEngineer features for binary classification.\n\nDevelop a logistic regression model for prediction.\n\nHandle class imbalance using SMOTE techniques.\n\nEvaluate classification performance using confusion matrix and metrics.\n\n\n\n\nKey Skills Gained\n\nLogistic regression modeling.\n\nData wrangling and preprocessing.\n\nBinary classification evaluation.\n\nSMOTE for class imbalance.\n\nR for machine learning pipelines.\n\n\n\n\n\nGroup Work with Colleagues Angel Delgado and Marvin Castrejon.\nEach Member of our group first loaded up the libraries and imported the Titanic dataset from our own directory where the dataset is housed.\n\nlibrary(rio)\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(SmartEDA)\nTitanic=import(\"C:/Users/bigem/OneDrive/Documents/1. MSDM - College/1. IBM 6800 Jae Jung/M14/M14/listings/Titanic.csv\") |&gt;\n  clean_names(\"upper_camel\") \n\nhead(Titanic)\n\n  Survived Pclass                        Name    Sex Age SiblingsSpousesAboard\n1        0      3      Mr. Owen Harris Braund   male  22                     1\n2        1      1   Mrs. John Bradley Cumings female  38                     1\n3        1      3       Miss. Laina Heikkinen female  26                     0\n4        1      1 Mrs. Jacques Heath Futrelle female  35                     1\n5        0      3     Mr. William Henry Allen   male  35                     0\n6        0      3             Mr. James Moran   male  27                     0\n  ParentsChildrenAboard FareInPounds\n1                     0       7.2500\n2                     0      71.2833\n3                     0       7.9250\n4                     0      53.1000\n5                     0       8.0500\n6                     0       8.4583\n\n\nIn the following code we downloaded the corrplot library and created the DataDeaths dataset where we selected Survive, Passenger class, Sex, and Age. We then mutated Survived and created a new column called SurvivedYes which is a 1 if the person survived, and the rest is 0. We then used Null to delete the original Survived column. We also mutated Male and followed the same format as the survived column. Using corrplot we were able to plot the data so we could visualize if the data was positively or negatively impacted by each variable in the dataset.\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.2\n\n\ncorrplot 0.92 loaded\n\nDataDeaths=Titanic |&gt;\n  select(Survived,Pclass,Sex,Age) |&gt;\n  mutate(SurvivedYes=ifelse(Survived==1,1,0), Survived=NULL) |&gt;\n  mutate(MaleYes=ifelse(Sex==\"male\",1,0), Sex=NULL) \ncorrplot(cor(DataDeaths))\n\n\n\n\n\n\n\n\nWe mutated the survived variable into a factor.\n\nDataDeaths=DataDeaths |&gt; \n  mutate(SurvivedYes=as.factor(SurvivedYes))\n\nWe set a random seed and then split the dataset into a 30/70 DataTrain and DataTest set. Our main focus was on survival as we set our factor into the strata.\n\nset.seed(789)\nSplit3070=initial_split(DataDeaths, prop=0.7,strata=SurvivedYes)\nDataTrain=training(Split3070)\nDataTest=testing(Split3070)\nhead(DataTrain)\n\n  Pclass Age SurvivedYes MaleYes\n1      3  22           0       1\n2      3  35           0       1\n3      3  27           0       1\n4      3   2           0       1\n5      3  20           0       1\n6      3  14           0       0\n\n\nWe created a recipe so that we could ultimately create a workflow. As can be seen, SurvivedYes is the dependent variable in our Recipe.\n\nRecipeDeaths=recipe(SurvivedYes~Pclass+MaleYes+Age, data=DataTrain)\n\nWe created a ModelDesign for our logistic regression and we set our engine and mode below.\n\nModelDesignLogistic=logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\nWe pieced together our WF and fit the DataTrain to our WFModel to see what outcome we would achieve.\n\nWFModelDeaths=workflow() |&gt;\n  add_recipe(RecipeDeaths) |&gt;\n  add_model(ModelDesignLogistic) |&gt;\n  fit(DataTrain)\n\ntidy(WFModelDeaths)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.68     0.553        8.47 2.49e-17\n2 Pclass       -1.20     0.148       -8.12 4.72e-16\n3 MaleYes      -2.62     0.224      -11.7  1.70e-31\n4 Age          -0.0295   0.00852     -3.47 5.27e- 4\n\n\nOur p-values for Pclass, MaleYes, and Age all prove to be significant with values &lt;0.05.\nWe now include the predictions in the data frame. pred_class shows if the passenger survived with 1. pred_0 shows the passengers percentage of belonging to the death rate. pred_1 shows the passengers survival rate.\n\nDataTestWithPred=augment(WFModelDeaths, new_data = DataTest)\nhead(DataTestWithPred)\n\n# A tibble: 6 × 7\n  .pred_class .pred_0 .pred_1 Pclass   Age SurvivedYes MaleYes\n  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n1 1             0.425  0.575       3    26 1                 0\n2 0             0.676  0.324       1    54 0                 1\n3 1             0.433  0.567       3    27 1                 0\n4 1             0.279  0.721       3     4 1                 0\n5 0             0.937  0.0628      3    39 0                 1\n6 1             0.343  0.657       2    55 1                 0\n\n\n\nconf_mat(DataTestWithPred, truth=SurvivedYes, estimate= .pred_class)\n\n          Truth\nPrediction   0   1\n         0 142  29\n         1  22  74\n\n\nTrue Positive (TP): 142 True Negative (TN): 74 False Positive (FP): 29 False Negative (FN): 22\n\nDDeaths=metric_set(accuracy, sensitivity, specificity)\nDDeaths(DataTestWithPred, truth=SurvivedYes, estimate= .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.809\n2 sensitivity binary         0.866\n3 specificity binary         0.718\n\n\nAcurracy: The model predicts the survival status correctly about 81% of the time. Sensitivity: The model correctly identifies about 87% of the actual survivors. Specificity: The model correctly identifies about 72% of the passengers who did not survive.\n\ncount(DataTrain, SurvivedYes)\n\n  SurvivedYes   n\n1           0 381\n2           1 239\n\n\nCount shows how many passengers survived 1 = 239, and how many passengers did not survive 0 = 381.\nIn this recipe we include step_smote() and then we rerun the workflow with DataTrain to see how our results have changed.\n\nlibrary(themis)\n\nWarning: package 'themis' was built under R version 4.3.3\n\nRecipeDeaths=recipe(SurvivedYes~Pclass+MaleYes+Age, data=DataTrain) |&gt;\n  step_smote(SurvivedYes, over_ratio=0.8)\n\n\nWFModelDeaths=workflow() |&gt;\n  add_recipe(RecipeDeaths) |&gt;\n  add_model(ModelDesignLogistic) |&gt;\n  fit(DataTrain)\n\nDataTestWithPred=augment(WFModelDeaths, new_data = DataTest)\n\nconf_mat(DataTestWithPred, truth=SurvivedYes, estimate= .pred_class)\n\n          Truth\nPrediction   0   1\n         0 134  27\n         1  30  76\n\nDDeaths=metric_set(accuracy, sensitivity, specificity)\nDDeaths(DataTestWithPred, truth=SurvivedYes, estimate= .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.787\n2 sensitivity binary         0.817\n3 specificity binary         0.738\n\n\nAs can be seen from the confusion matrix and the metrics of the sample, our outcome has changed. True Positive (TP): 134 True Negative (TN): 76 False Positive (FP): 27 False Negative (FN): 30 Accuracy: The model predicts the survival status correctly about 79% of the time. Sensitivity: The model correctly identifies about 82% of the actual survivors. Specificity: The model correctly identifies about 74% of the passengers who did not survive."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Eric Garcia",
    "section": "",
    "text": "I’m a Digital Marketing Specialist and Analyst with a passion for turning data-driven insights into strategic actions that drive growth. With a strong foundation in SEO, SEM, content strategy, and performance analytics, I help businesses enhance their online presence, optimize marketing efforts, and connect with their target audiences more effectively. My approach combines analytical precision with creative problem-solving to deliver measurable results in fast-paced digital environments."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Hi, I’m Eric Garcia",
    "section": "About Me",
    "text": "About Me\nI hold a Master of Science in Digital Marketing Degree from California State Polytechnic University, Pomona. Some hobbies of mine include golf, taking my dog to the dog park, exercising, and video games. I am joining the Amazon workforce soon, so stay tuned…"
  },
  {
    "objectID": "index.html#lets-connect",
    "href": "index.html#lets-connect",
    "title": "Hi, I’m Eric Garcia",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nYou can email me or connect on LinkedIn.\n\nWelcome to my portfolio."
  },
  {
    "objectID": "listings/02-seo-strategy-model.html",
    "href": "listings/02-seo-strategy-model.html",
    "title": "Sleep Health SEO Strategy Model",
    "section": "",
    "text": "Overview\nIn this project, we analyzed the Sleep Health and Lifestyle dataset to model sleep quality and inform SEO content strategies related to wellness topics. Using regression and DAG models, we examined how stress, exercise, heart rate, and BMI relate to sleep quality. The findings guided content creation for keyword-rich, data-backed wellness pages.\n\n\n\n\nLearning Outcomes\n\nTransform and engineer categorical health data for modeling.\n\nUse DAGs to define causal assumptions in wellness data.\n\nPerform EDA and correlation analysis for SEO-driven topics.\n\nBuild regression models to explain sleep quality outcomes.\n\nConnect data-driven health insights to content strategy.\n\n\n\n\nKey Skills Gained\n\nDAG modeling with ggdag\n\nSEO content mapping from data insights\n\nExploratory data analysis (EDA)\n\nLogistic and linear regression in R\n\nData storytelling with health metrics\n\n\n\n\n\nGroup Project with Kiki Shimomae.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(rio)\nlibrary(janitor)\n\nSleepHealth &lt;- import(\"C:/Users/bigem/OneDrive/Documents/1. MSDM - College/1. IBM 6800 Jae Jung/M14/M14/listings/Sleep_health_and_lifestyle_dataset.csv\") |&gt;\n  clean_names(\"upper_camel\")  # This will make your column names neat and camel-case\n\nhead(SleepHealth)\n\n  PersonId Gender Age           Occupation SleepDuration QualityOfSleep\n1        1   Male  27    Software Engineer           6.1              6\n2        2   Male  28               Doctor           6.2              6\n3        3   Male  28               Doctor           6.2              6\n4        4   Male  28 Sales Representative           5.9              4\n5        5   Male  28 Sales Representative           5.9              4\n6        6   Male  28    Software Engineer           5.9              4\n  PhysicalActivityLevel StressLevel BmiCategory BloodPressure HeartRate\n1                    42           6  Overweight        126/83        77\n2                    60           8      Normal        125/80        75\n3                    60           8      Normal        125/80        75\n4                    30           8       Obese        140/90        85\n5                    30           8       Obese        140/90        85\n6                    30           8       Obese        140/90        85\n  DailySteps SleepDisorder\n1       4200          None\n2      10000          None\n3      10000          None\n4       3000   Sleep Apnea\n5       3000   Sleep Apnea\n6       3000      Insomnia\n\n\n\nlibrary(ggdag) \n\nWarning: package 'ggdag' was built under R version 4.3.3\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ndagify(SleepQuality~Stress+Exercise+HeartRate+BMI,\n       BMI~Exercise) |&gt; \nggdag(node_size = 22) + theme_dag_gray_grid()\n\n\n\n\n\n\n\n\n\nlibrary(wooldridge)\n\n# 1. change all the variable to numeric\n\nSexType &lt;- c(Male = 0, Female = 1)\nSleepHealth$Sex &lt;- SexType[SleepHealth$Gender] \n# Changing Gender (chr variable) to numeric variable. Male = 0, female = 1 \ncolnames(SleepHealth)\n\n [1] \"PersonId\"              \"Gender\"                \"Age\"                  \n [4] \"Occupation\"            \"SleepDuration\"         \"QualityOfSleep\"       \n [7] \"PhysicalActivityLevel\" \"StressLevel\"           \"BmiCategory\"          \n[10] \"BloodPressure\"         \"HeartRate\"             \"DailySteps\"           \n[13] \"SleepDisorder\"         \"Sex\"                  \n\nSleepHealth = SleepHealth |&gt;\n    mutate(`BmiCategory` = ifelse(`BmiCategory` == \"Normal Weight\", \"Normal\", `BmiCategory`))\n# combining both Normal & Normal Weight into one category of \"Normal\" \n\nBMIType &lt;-c(Normal = 0, Overweight = 1, Obese = 2)\nSleepHealth$BMI &lt;- BMIType[SleepHealth$`BmiCategory`]\n# Changing BMI Category (chr variable) to numeric variable. \n# Normal = 0, Overweight = 1, Obese = 2) \n\n\n# changing variable name to make it easy & short\n\nSleepHealthData &lt;- SleepHealth |&gt;\n  select(\n    SleepQuality = QualityOfSleep,\n    Stress = StressLevel,\n    Exercise = PhysicalActivityLevel,\n    HeartRate,\n    Age,\n    SleepDuration,\n    DailySteps,\n    Sex,\n    BMI = BmiCategory\n  ) |&gt;\n  clean_names(\"upper_camel\")\n\n\nlibrary(SmartEDA)\n\nWarning: package 'SmartEDA' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nExpReport(SleepHealthData, op_file = \"EDAReport.html\")\n\n\n\nprocessing file: report_tmp_1.Rmd\n\n\n\n  |                                                                 \n  |                                                           |   0%\n  |                                                                 \n  |..                                                         |   3%           \n  |                                                                 \n  |....                                                       |   6% [setup]   \n  |                                                                 \n  |......                                                     |  10%           \n  |                                                                 \n  |........                                                   |  13% [od_1]    \n  |                                                                 \n  |..........                                                 |  16%           \n  |                                                                 \n  |...........                                                |  19% [od_2]    \n  |                                                                 \n  |.............                                              |  23%           \n  |                                                                 \n  |...............                                            |  26% [od_3]    \n  |                                                                 \n  |.................                                          |  29%           \n  |                                                                 \n  |...................                                        |  32% [od_32]   \n  |                                                                 \n  |.....................                                      |  35%           \n  |                                                                 \n  |.......................                                    |  39% [snv_all] \n  |                                                                 \n  |.........................                                  |  42%           \n  |                                                                 \n  |...........................                                |  45% [snv_2]   \n  |                                                                 \n  |.............................                              |  48%           \n  |                                                                 \n  |..............................                             |  52% [snv_2.1] \n  |                                                                 \n  |................................                           |  55%           \n  |                                                                 \n  |..................................                         |  58% [snv2_new]\n  |                                                                 \n  |....................................                       |  61%           \n  |                                                                 \n  |......................................                     |  65% [snv2]    \n  |                                                                 \n  |........................................                   |  68%           \n  |                                                                 \n  |..........................................                 |  71% [snv22_1] \n  |                                                                 \n  |............................................               |  74%           \n  |                                                                 \n  |..............................................             |  77% [eda_4]   \n  |                                                                 \n  |................................................           |  81%           \n  |                                                                 \n  |.................................................          |  84% [e4.1]    \n  |                                                                 \n  |...................................................        |  87%           \n  |                                                                 \n  |.....................................................      |  90% [e4.1.1]  \n  |                                                                 \n  |.......................................................    |  94%           \n  |                                                                 \n  |.........................................................  |  97% [bp1]     \n  |                                                                 \n  |...........................................................| 100%           \n                                                                                                     \n\n\noutput file: C:/Users/bigem/OneDrive/Documents/1. MSDM - College/1. IBM 6800 Jae Jung/M14/M14/listings/report_tmp_1.knit.md\n\n\n\"C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/pandoc\" +RTS -K512m -RTS \"C:\\Users\\bigem\\OneDrive\\DOCUME~1\\15A01~1.MSD\\15067~1.IBM\\M14\\M14\\listings\\REPORT~1.MD\" --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output pandoc8e7c6ce7cee.html --lua-filter \"C:\\Users\\bigem\\AppData\\Local\\R\\win-library\\4.3\\rmarkdown\\rmarkdown\\lua\\pagebreak.lua\" --lua-filter \"C:\\Users\\bigem\\AppData\\Local\\R\\win-library\\4.3\\rmarkdown\\rmarkdown\\lua\\latex-div.lua\" --embed-resources --standalone --variable bs3=TRUE --section-divs --table-of-contents --toc-depth 6 --template \"C:\\Users\\bigem\\AppData\\Local\\R\\win-library\\4.3\\rmarkdown\\rmd\\h\\default.html\" --no-highlight --variable highlightjs=1 --variable theme=cerulean --mathjax --variable \"mathjax-url=https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" --include-in-header \"C:\\Users\\bigem\\AppData\\Local\\Temp\\RtmpOo2xDG\\rmarkdown-str8e7c997b02.html\" \n\n\n\nOutput created: EDAReport.html\n\n\n\n\nReport is generated at \"C:/Users/bigem/OneDrive/Documents/1. MSDM - College/1. IBM 6800 Jae Jung/M14/M14/listings/EDAReport.html\".\n\n\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.2\n\n\ncorrplot 0.92 loaded\n\n# Filter numeric columns only\nnumeric_vars &lt;- SleepHealthData |&gt;\n  dplyr::select(where(is.numeric))\n\nCorMatrix &lt;- cor(numeric_vars, use = \"complete.obs\")\ncorrplot(CorMatrix)\n\n\n\n\n\n\n\n\n\nSleepData &lt;- SleepHealthData |&gt;\n  select(SleepQuality, Stress, Exercise, HeartRate, Bmi)"
  },
  {
    "objectID": "listings/04-kknn-model.html",
    "href": "listings/04-kknn-model.html",
    "title": "Tuning a kNearest Neighbor Model",
    "section": "",
    "text": "Overview\nThis project used a k-Nearest Neighbor (KNN) classification algorithm to predict wine color (Red or White) based on chemical attributes. A structured workflow was created to preprocess the data, tune hyperparameters, validate model performance, and identify the most accurate model configuration.\n\n\n\n\nLearning Outcomes\n\nPrepare and normalize data using preprocessing recipes.\n\nImplement classification models using the tidymodels framework.\n\nApply cross-validation and hyperparameter tuning for model selection.\n\nEvaluate accuracy, specificity, and sensitivity of model predictions.\n\nBuild a reproducible ML pipeline in R.\n\n\n\n\nKey Skills Gained\n\nData preprocessing with recipes and janitor.\n\nModel training and tuning with tidymodels and kknn.\n\nCross-validation with stratified folds.\n\nClassification performance evaluation.\n\nAutomated ML workflows in R.\n\n\n\n\n\nLibraries and Loading Data\n\nlibrary(tidymodels)\nlibrary(rio)\nlibrary(janitor)\nlibrary(kknn)\nDataWine=import(\"https://ai.lange-analytics.com/data/WineData.rds\") |&gt; \n         clean_names(\"upper_camel\") |&gt; \n         rename(Sulfur=TotalSulfurDioxide) |&gt; \n         mutate(WineColor=as.factor(WineColor)) \n\n\n\nStep 1: Split Data in Training and Testing Data\nWe split the data into training and testing data. DataTest has 30% of the data in the Wine dataset, and DataTrain has 70% of the data in the wine dataset.\n\nset.seed(876)\nSplit7030=initial_split(DataWine, prop=0.7, strata=WineColor)\nDataTrain=training(Split7030)\nDataTest=testing(Split7030)\n\nhead(DataTrain)\n\n  WineColor Acidity VolatileAcidity CitricAcid ResidualSugar Chlorides\n1       red    10.8           0.320       0.44           1.6     0.063\n2       red     6.7           0.855       0.02           1.9     0.064\n3       red     7.5           0.380       0.57           2.3     0.106\n4       red     7.1           0.270       0.60           2.1     0.074\n5       red     8.0           0.580       0.28           3.2     0.066\n6       red     7.6           0.400       0.29           1.9     0.078\n  FreeSulfurDioxide Sulfur Density   PH Sulphates Alcohol Quality\n1                16     37 0.99850 3.22      0.78   10.00       6\n2                29     38 0.99472 3.30      0.56   10.75       6\n3                 5     12 0.99605 3.36      0.55   11.40       6\n4                17     25 0.99814 3.38      0.72   10.60       6\n5                21    114 0.99730 3.22      0.54    9.40       6\n6                29     66 0.99710 3.45      0.59    9.50       6\n\n\n\n\nStep 2 - Create a Recipe:\nWe created the recipe by following the steps from the book and use step_rm to remove the variable “Quality” since it is not related to Wine color.\n\nRecipe=recipe(WineColor~., data=DataTrain) |&gt;\n       step_rm(Quality) |&gt;\n       step_normalize(all_predictors())\n\nprint(Recipe)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 12\n\n\n\n\n\n── Operations \n\n\n• Variables removed: Quality\n\n\n• Centering and scaling for: all_predictors()\n\n\n\n\nStep 3 - Create a Model Design:\nWe use the tuning function and assign it as a placeholder so that we may assign specific numerical values later on.\n\nModelDesign=nearest_neighbor(neighbors=tune()) |&gt;\n               set_engine(\"kknn\") |&gt; \n               set_mode(\"classification\")\nprint(ModelDesign)\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\n\n\nStep 4 - Add the Recipe and the Model Design to a Workflow:\nWe add the recipe and model design and execute the workflow.\n\nTuneWFModel=workflow() |&gt;\n            add_recipe(Recipe) |&gt;\n            add_model(ModelDesign)\nprint(TuneWFModel)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_rm()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\n\n\nStep 5 - Create a Hyper-Parameter Grid:\nWe add 1:15 to the hyper-parameter grid so that these numbers can be tried out by the time we are at step 7.\n\nParGrid=data.frame(neighbors=c(1:15))\nprint(ParGrid)\n\n   neighbors\n1          1\n2          2\n3          3\n4          4\n5          5\n6          6\n7          7\n8          8\n9          9\n10        10\n11        11\n12        12\n13        13\n14        14\n15        15\n\n\n\n\nStep 6 - Creating Resamples for Cross-Validation:\nWe substitue the number 5 so that we can create the five folds for samples later on.\n\nset.seed(123)\nFoldsForTuning=vfold_cv(DataTrain, v=5,\n                        strata=WineColor)\nprint(FoldsForTuning)\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits             id   \n  &lt;list&gt;             &lt;chr&gt;\n1 &lt;split [1790/448]&gt; Fold1\n2 &lt;split [1790/448]&gt; Fold2\n3 &lt;split [1790/448]&gt; Fold3\n4 &lt;split [1790/448]&gt; Fold4\n5 &lt;split [1792/446]&gt; Fold5\n\n\n\n\nStep 7 - Tune the Workflow and Train All Models:\nWe incorporate three metrics into the data and are able to see the three charts that are displayed. A k of 1-4 has the best accuracy and should be used to yield the best results\n\nTuneResults=tune_grid(TuneWFModel, \n                      resamples=FoldsForTuning, \n                      grid=ParGrid, \n                      metrics=metric_set(accuracy, specificity, sensitivity))\nautoplot(TuneResults)\n\n\n\n\n\n\n\n\n\n\nStep 8 - Extract the Best Hyper-Parameter(s):\nAll assessment results for the specified metrics. We substitute accuracy into the dataset. People who have the updated version need to use “metric =” for this coding section.\n\nBestHyperPar &lt;- select_best(TuneResults, metric = \"accuracy\")\nprint(BestHyperPar)\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;int&gt; &lt;chr&gt;                \n1         1 Preprocessor1_Model01\n\n\n\n\nStep 9 - Finalize and Train the Best Workflow Model:\nExecute the final workflow to set the hyper-parameter to neighbors=1.\n\nWFModelBest=TuneWFModel |&gt;\n            finalize_workflow(BestHyperPar) |&gt;\n            fit(DataTrain)\n\nprint(WFModelBest)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_rm()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(1L,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.009383378\nBest kernel: optimal\nBest k: 1\n\n\n\n\nStep 10: Assess Prediction Quality Based on the Testing Data:\nOut of 480 White Wines only 7 of them were incorrectly classified.\n\nDataTestWithPredBestModel=augment(WFModelBest, DataTest)\nconf_mat(DataTestWithPredBestModel, truth=WineColor,\n         estimate=.pred_class)\n\n          Truth\nPrediction red white\n     red   474     7\n     white   6   473"
  },
  {
    "objectID": "marketing-analytics.html",
    "href": "marketing-analytics.html",
    "title": "Marketing Analytics",
    "section": "",
    "text": "Below are selected projects that highlight my experience applying marketing analytics to real-world challenges. These projects reflect my ability to combine statistical tools, campaign data, and strategic thinking to inform decisions and drive results.\nWhether developing dashboards, analyzing campaign performance, or uncovering insights through SEO and web analytics, each project is rooted in turning complex marketing data into clear, actionable strategies."
  },
  {
    "objectID": "marketing-analytics.html#what-youll-find",
    "href": "marketing-analytics.html#what-youll-find",
    "title": "Marketing Analytics",
    "section": "What You’ll Find",
    "text": "What You’ll Find\n\nCampaign Performance Analysis\n\nDive into a thorough evaluation of digital marketing campaigns, assessing ROI, conversion rates, and engagement across platforms like Google Ads, Facebook, and organic search. This involves time-series analysis, cohort tracking, and A/B test comparisons to isolate the most effective strategies and ensure data-driven budget allocation.\nData Visualization and Reporting\n\nRaw data is transformed into meaningful, interactive dashboards and reports using tools such as Tableau, Power BI, or Python libraries like Plotly and Seaborn. These visualizations not only present trends clearly but also enable stakeholders to make fast, informed decisions based on real-time insights and historical performance.\nAdvanced Marketing Mix Modeling & Channel Attribution\n\nExplore the nuanced contributions of each marketing channel through multi-touch attribution models, regression-based marketing mix models (MMM), and cross-channel synergy analysis. This includes scenario simulations to predict outcomes based on shifts in media spending or campaign structure.\nCustomer & Market Behavior Insights\n\nUtilize segmentation, clustering, and predictive modeling to understand customer lifecycles, preferences, and churn risk. Leverage demographic and behavioral data to tailor campaigns, improve user journeys, and uncover emerging market trends. This work guides personalization strategies and supports long-term growth planning."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "You can download or view a PDF version of my résumé here.\n\nThis browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"resume/Resume.pdf\"&gt;Download Résumé&lt;/a&gt;"
  },
  {
    "objectID": "resume.html#résumé",
    "href": "resume.html#résumé",
    "title": "Resume",
    "section": "",
    "text": "You can download or view a PDF version of my résumé here.\n\nThis browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"resume/Resume.pdf\"&gt;Download Résumé&lt;/a&gt;"
  }
]